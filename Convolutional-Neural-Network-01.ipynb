{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "img_width, img_height = 224, 224\n",
    "image_size = img_width* img_height\n",
    "LEARNING_RATE = 0.0005\n",
    "EPSILON = 1e-4\n",
    "LABEL_NUM = 2\n",
    "TRAIN_DATASET = 1000\n",
    "TEST_DATASET = 2500\n",
    "BATCH_SIZE = 32\n",
    "epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data directory\n",
    "def read_data(data_dir):\n",
    "    data_directory = data_dir\n",
    "    \n",
    "    filename ={'train':[],'validation':[]}\n",
    "    dataset = ['train','validation']\n",
    "\n",
    "    for dataset_name in dataset:\n",
    "        for subFile in os.listdir(data_directory):\n",
    "            if dataset_name in subFile and re.match(r'^.*\\.tfrecords$', subFile):\n",
    "                filename[dataset_name].append(data_directory+\"/\"+subFile)\n",
    "#     print(filename)        \n",
    "#     training_filenames = filename['train']\n",
    "#     validation_filenames = filename['validation']\n",
    "    \n",
    "    return filename['train'] , filename['validation'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(serialized):\n",
    "    features = { 'label': tf.FixedLenFeature([], tf.int64),\n",
    "               'text_label': tf.FixedLenFeature([], tf.string),\n",
    "               'image': tf.FixedLenFeature([], tf.string)\n",
    "               }\n",
    "\n",
    "    parsed_example = tf.parse_single_example(serialized=serialized,\n",
    "                                             features=features)\n",
    "\n",
    "    image_raw = parsed_example['image']\n",
    "\n",
    "    image = tf.decode_raw(image_raw, tf.uint8)\n",
    "    image = tf.reshape(image, [img_width, img_height, 3])\n",
    "#     image = tf.image.resize_images(\n",
    "#                 image,\n",
    "#                 [64, 64]\n",
    "#             )\n",
    "#     image = tf.image.rgb_to_grayscale(image, name=None)\n",
    "    image = tf.cast(image, tf.float32)    \n",
    "    image = image / 255.\n",
    "    \n",
    "    label = parsed_example['label']\n",
    "    label = tf.one_hot(label,LABEL_NUM)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "#     label = tf.reshape(label, [1])\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(filename , BUFFER_SIZE = 25000):\n",
    "    dataset = tf.data.TFRecordDataset(filename)\n",
    "    dataset = dataset.map(parse)\n",
    "    dataset = dataset.shuffle(buffer_size=BUFFER_SIZE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(self):\n",
    "        self.cost = None\n",
    "        self.accuracy = None\n",
    "        self.prediction = None\n",
    "        self.logits = None\n",
    "        self.global_step = None \n",
    "        \n",
    "        \n",
    "#         print(\"CNN\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        #model VGG16\n",
    "        x = tf.layers.conv2d(inputs = x,filters = 32,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu) \n",
    "#         x = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "        x = tf.layers.max_pooling2d(x, pool_size = [2,2], strides =[2,2])\n",
    "\n",
    "#         x = tf.layers.conv2d(inputs = x,filters = 128,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "        x = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "        x = tf.layers.max_pooling2d(x, pool_size = [2,2], strides =[2,2])\n",
    "        \n",
    "#         x = tf.layers.conv2d(inputs = x,filters = 256,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "#         x = tf.layers.conv2d(inputs = x,filters = 256,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "#         x = tf.layers.conv2d(inputs = x,filters = 128,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "#         x = tf.layers.max_pooling2d(x, pool_size = [2,2], strides =[2,2])\n",
    "\n",
    "\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, 128, activation=tf.nn.relu)       \n",
    "        x = tf.layers.dropout(x,rate=0.5)\n",
    "        x = tf.layers.dense(x, 2, activation=tf.nn.sigmoid, use_bias = True)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def train(self, x, y, learning_rate = 0.01, epsilon = 1e-10):\n",
    "        self.logits = self.predict(x)\n",
    "        \n",
    "        with tf.name_scope(\"cost\"):\n",
    "            self.cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = self.logits)  , name='cost')\n",
    "            tf.summary.scalar(\"cost\", self.cost)\n",
    "        \n",
    "        self.global_step = tf.train.get_or_create_global_step()   \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, epsilon=epsilon)\n",
    "        optimize = optimizer.minimize(self.cost , global_step = self.global_step)\n",
    "        accuracy = self.validate(self.logits, y)\n",
    "        return self.cost, accuracy, optimize, self.global_step\n",
    "        \n",
    "    def validate(self, logits, y):         \n",
    "        with tf.name_scope(\"validate\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy )\n",
    "            tf.summary.histogram(\"accuracy\", self.accuracy )\n",
    "            \n",
    "        return self.accuracy\n",
    "    \n",
    "    def test(self, x, y):\n",
    "        self.logits = self.predict(x)\n",
    "        accuracy = validate(self.logits, y)\n",
    "        return accuracy\n",
    "        \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = train_dataset.get_next()\n",
    "# x = tf.layers.conv2d(inputs = x,filters = 32,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu) \n",
    "# x = tf.layers.max_pooling2d(x, pool_size = 2, strides = 2)\n",
    "# x = tf.layers.conv2d(inputs = x,filters = 32,kernel_size = [3,3], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "# x = tf.layers.max_pooling2d(x, pool_size = [2,2], strides =[2,2])\n",
    "\n",
    "# x = tf.layers.conv2d(inputs = x,filters = 64,kernel_size = [5,5], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "# x = tf.layers.max_pooling2d(x, pool_size = [2,2], strides =[2,2])\n",
    "\n",
    "# x = tf.layers.conv2d(inputs = x,filters = 128,kernel_size = [5,5], strides=1, padding='SAME', activation=tf.nn.relu)\n",
    "# x = tf.layers.max_pooling2d(x, pool_size = [2,2], strides =[2,2])\n",
    "\n",
    "# x = tf.layers.flatten(x)\n",
    "# x = tf.layers.dense(x, 128, activation=tf.nn.relu)\n",
    "# x = tf.layers.dropout(x,rate=0.1)\n",
    "# x = tf.layers.dense(x, 2, activation=tf.nn.sigmoid, use_bias = True)\n",
    "\n",
    "# logits = x\n",
    "# tf.summary.histogram(\"logits\", logits)\n",
    "# with tf.name_scope(\"cost\"):\n",
    "#     cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = logits)  , name='cost')\n",
    "#     tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "# global_step = tf.train.get_or_create_global_step()   \n",
    "# with tf.name_scope(\"train\"):\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE, epsilon = EPSILON)\n",
    "#     optimize = optimizer.minimize(cost , global_step = global_step)\n",
    "    \n",
    "# with tf.name_scope(\"validate\"):\n",
    "#     correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     tf.summary.scalar(\"accuracy\", accuracy)\n",
    "#     tf.summary.histogram(\"accuracy\", accuracy)\n",
    "\n",
    "# merge = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/train-3.tfrecords', '../data/train-2.tfrecords', '../data/train-1.tfrecords', '../data/train-0.tfrecords']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"    \n",
    "training_filenames, validation_filenames= read_data(data_dir)\n",
    "train_dataset = input_pipeline(training_filenames)\n",
    "test_dataset = input_pipeline(validation_filenames)\n",
    "\n",
    "print(training_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CNN()\n",
    "x_train, y_train = train_dataset.get_next()\n",
    "x_test, y_test = train_dataset.get_next()\n",
    "\n",
    "cost , accuracy, optimize, global_step = clf.train(x = x_train, y = y_train, learning_rate = LEARNING_RATE, epsilon = EPSILON)\n",
    "test_accuracy = clf.validate(clf.predict(x_test), y_test)\n",
    "merge = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 | TRAIN Loss 0.695614 , Accuracy 0.518145 | TEST Accuracy 0.500801\n",
      "Epoch : 2 | TRAIN Loss 0.693119 , Accuracy 0.516129 | TEST Accuracy 0.506010\n",
      "Epoch : 3 | TRAIN Loss 0.693078 , Accuracy 0.521169 | TEST Accuracy 0.506410\n",
      "Epoch : 4 | TRAIN Loss 0.693035 , Accuracy 0.498992 | TEST Accuracy 0.496795\n",
      "Epoch : 5 | TRAIN Loss 0.692930 , Accuracy 0.486895 | TEST Accuracy 0.520433\n",
      "Epoch : 6 | TRAIN Loss 0.692391 , Accuracy 0.502016 | TEST Accuracy 0.503205\n",
      "Epoch : 7 | TRAIN Loss 0.693122 , Accuracy 0.489919 | TEST Accuracy 0.502003\n",
      "Epoch : 8 | TRAIN Loss 0.693132 , Accuracy 0.503024 | TEST Accuracy 0.520032\n",
      "Epoch : 9 | TRAIN Loss 0.693098 , Accuracy 0.518145 | TEST Accuracy 0.511218\n",
      "Epoch : 10 | TRAIN Loss 0.693002 , Accuracy 0.500000 | TEST Accuracy 0.517628\n",
      "Epoch : 11 | TRAIN Loss 0.692878 , Accuracy 0.480847 | TEST Accuracy 0.512821\n",
      "Epoch : 12 | TRAIN Loss 0.688677 , Accuracy 0.534274 | TEST Accuracy 0.526843\n",
      "Epoch : 13 | TRAIN Loss 0.691075 , Accuracy 0.473790 | TEST Accuracy 0.519231\n",
      "Epoch : 14 | TRAIN Loss 0.690145 , Accuracy 0.494960 | TEST Accuracy 0.505208\n",
      "Epoch : 15 | TRAIN Loss 0.689127 , Accuracy 0.477823 | TEST Accuracy 0.505208\n",
      "Epoch : 16 | TRAIN Loss 0.688353 , Accuracy 0.503024 | TEST Accuracy 0.520433\n",
      "Epoch : 17 | TRAIN Loss 0.691868 , Accuracy 0.513105 | TEST Accuracy 0.497596\n",
      "Epoch : 18 | TRAIN Loss 0.692012 , Accuracy 0.509073 | TEST Accuracy 0.504006\n",
      "Epoch : 19 | TRAIN Loss 0.686747 , Accuracy 0.468750 | TEST Accuracy 0.515224\n",
      "Epoch : 20 | TRAIN Loss 0.684830 , Accuracy 0.521169 | TEST Accuracy 0.507612\n",
      "Epoch : 21 | TRAIN Loss 0.678299 , Accuracy 0.514113 | TEST Accuracy 0.510417\n",
      "Epoch : 22 | TRAIN Loss 0.681431 , Accuracy 0.505040 | TEST Accuracy 0.515224\n",
      "Epoch : 23 | TRAIN Loss 0.685274 , Accuracy 0.490927 | TEST Accuracy 0.511619\n",
      "Epoch : 24 | TRAIN Loss 0.689779 , Accuracy 0.515121 | TEST Accuracy 0.510417\n",
      "Epoch : 25 | TRAIN Loss 0.682100 , Accuracy 0.501008 | TEST Accuracy 0.500401\n",
      "Epoch : 26 | TRAIN Loss 0.680843 , Accuracy 0.504032 | TEST Accuracy 0.518830\n",
      "Epoch : 27 | TRAIN Loss 0.682416 , Accuracy 0.479839 | TEST Accuracy 0.515224\n",
      "Epoch : 28 | TRAIN Loss 0.679986 , Accuracy 0.513105 | TEST Accuracy 0.525641\n",
      "Epoch : 29 | TRAIN Loss 0.678627 , Accuracy 0.501008 | TEST Accuracy 0.513622\n",
      "Epoch : 30 | TRAIN Loss 0.681344 , Accuracy 0.514113 | TEST Accuracy 0.516827\n",
      "Epoch : 31 | TRAIN Loss 0.675908 , Accuracy 0.516129 | TEST Accuracy 0.522837\n",
      "Epoch : 32 | TRAIN Loss 0.672101 , Accuracy 0.510081 | TEST Accuracy 0.514423\n",
      "Epoch : 33 | TRAIN Loss 0.679364 , Accuracy 0.491935 | TEST Accuracy 0.496795\n",
      "Epoch : 34 | TRAIN Loss 0.670050 , Accuracy 0.523185 | TEST Accuracy 0.512420\n",
      "Epoch : 35 | TRAIN Loss 0.676467 , Accuracy 0.524194 | TEST Accuracy 0.528446\n",
      "Epoch : 36 | TRAIN Loss 0.675737 , Accuracy 0.535282 | TEST Accuracy 0.500000\n",
      "Epoch : 37 | TRAIN Loss 0.673006 , Accuracy 0.496976 | TEST Accuracy 0.502804\n",
      "Epoch : 38 | TRAIN Loss 0.665876 , Accuracy 0.533266 | TEST Accuracy 0.508013\n",
      "Epoch : 39 | TRAIN Loss 0.670933 , Accuracy 0.526210 | TEST Accuracy 0.506410\n",
      "Epoch : 40 | TRAIN Loss 0.669332 , Accuracy 0.547379 | TEST Accuracy 0.517628\n",
      "Epoch : 41 | TRAIN Loss 0.668435 , Accuracy 0.533266 | TEST Accuracy 0.519631\n",
      "Epoch : 42 | TRAIN Loss 0.673543 , Accuracy 0.585685 | TEST Accuracy 0.505609\n",
      "Epoch : 43 | TRAIN Loss 0.669181 , Accuracy 0.559476 | TEST Accuracy 0.532853\n",
      "Epoch : 44 | TRAIN Loss 0.665158 , Accuracy 0.587702 | TEST Accuracy 0.491987\n",
      "Epoch : 45 | TRAIN Loss 0.658576 , Accuracy 0.599798 | TEST Accuracy 0.514824\n",
      "Epoch : 46 | TRAIN Loss 0.670251 , Accuracy 0.627016 | TEST Accuracy 0.514423\n",
      "Epoch : 47 | TRAIN Loss 0.664712 , Accuracy 0.669355 | TEST Accuracy 0.499599\n",
      "Epoch : 48 | TRAIN Loss 0.636871 , Accuracy 0.723790 | TEST Accuracy 0.505609\n",
      "Epoch : 49 | TRAIN Loss 0.643968 , Accuracy 0.707661 | TEST Accuracy 0.491587\n",
      "Epoch : 50 | TRAIN Loss 0.638163 , Accuracy 0.705645 | TEST Accuracy 0.534856\n"
     ]
    }
   ],
   "source": [
    "ACC_TEST , ACC_TRAIN = [],[]\n",
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter('./logs/train', sess.graph)\n",
    "    test_writer =  tf.summary.FileWriter('./logs/test', sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    for i in range(epoch):        \n",
    "        total_loss, total_acc = 0 ,0\n",
    "        for x in range(0, (TRAIN_DATASET//BATCH_SIZE)* BATCH_SIZE, BATCH_SIZE):            \n",
    "            if x == 0:\n",
    "                loss, acc, _, summary, global_step_val= sess.run([cost, accuracy, optimize, merge, global_step])\n",
    "                train_writer.add_summary(summary, global_step_val)    \n",
    "            else:\n",
    "                loss, acc, _= sess.run([cost, accuracy, optimize])\n",
    "                \n",
    "            total_loss += loss\n",
    "            total_acc += acc    \n",
    "        \n",
    "\n",
    "        total_loss /= (TRAIN_DATASET // BATCH_SIZE) \n",
    "        total_acc /= (TRAIN_DATASET // BATCH_SIZE)\n",
    "        ACC_TRAIN.append(total_acc)\n",
    "        \n",
    "        total_acc = 0 \n",
    "        for x in range(0, (TEST_DATASET//BATCH_SIZE)* BATCH_SIZE, BATCH_SIZE):\n",
    "            acc, summary, global_step_val= sess.run([test_accuracy, merge, global_step])\n",
    "            test_writer.add_summary(summary, global_step_val)   \n",
    "                \n",
    "            total_acc += acc    \n",
    "        \n",
    "        total_acc /= (TEST_DATASET // BATCH_SIZE)\n",
    "        ACC_TEST.append(total_acc)\n",
    "        print(\"Epoch : %d | TRAIN Loss %f , Accuracy %f | TEST Accuracy %f\"% ((i+1),total_loss, ACC_TRAIN[-1], ACC_TEST[-1]))  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
